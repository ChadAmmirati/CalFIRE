

Development Challenge: Space-Based Data Acquisition, Storage, and Dissemination
-------------------------------------------------------------------------------

In the rapidly evolving domain of space-based data acquisition, the design and implementation of a robust data architecture present multifaceted challenges that must be addressed to ensure seamless collection, storage, processing, and analysis of data. As organizations utilize satellite and space sensor data, overcoming these challenges is imperative to harness the full potential of space-based information.

CAL FIRE, the California Department of Forestry and Fire Protection, stands at the forefront of needing such advanced data architectures to monitor, manage, and mitigate wildfires across the state.

As CAL FIRE integrates satellite and space sensor data into their operations, they face three critical challenges:

1.  Diversity and Volume of Data Sources and Ingestion Mechanisms: Requires efficient and scalable solutions to handle real-time data streams.

2.  Data Storage and Processing Layers: Must manage vast amounts of data while ensuring data integrity, security, and accessibility.

3.  Data Consumption and Analytics Layers: Need to facilitate advanced analytics and decision-making processes to enable effective wildfire response.

These challenges highlight the necessity for a comprehensive and resilient data architecture to support CAL FIRE's mission.

* * * * *

Challenge 1: Data Sources and Ingestion Mechanisms
--------------------------------------------------

Objective
---------

To architect, design, develop, and prototype a versatile data ingestion mechanism capable of handling batch, real-time, and streaming data from various sources, ensuring minimal latency and maximum fidelity.

Key Requirements
----------------

-   Support for processing batch data, real-time streams, and continuous inputs.

-   Minimal latency and maximum fidelity in data handling.

-   Accommodation of structured, semi-structured, and unstructured data sources.

-   Implementation of error handling and data validation for high data quality.

-   Scalable data pipelines adaptable to varying data loads.

* * * * *

Deliverables
------------

Submissions will be accepted from Friday, August 22, 2025, until midnight on Sunday, October 26, 2025.

Core Technical Deliverables
---------------------------

-   Architectural Blueprint

    -   High-level system architecture diagram

    -   Data flow and component interaction overview

    -   Justification of chosen technologies for latency/fidelity balance

-   Data Ingestion Prototype

    -   Source adapters/connectors for batch, real-time, and streaming inputs

    -   Support for structured, semi-structured, unstructured formats

    -   Scalable pipeline implementation

-   Latency & Fidelity Metrics Dashboard

    -   Visualization of data processing latency across ingestion modes

    -   Fidelity checks and validation results

Reliability & Scalability Assets
--------------------------------

-   Error Handling & Validation Framework

    -   Data quality assurance modules

    -   Protocols for schema validation, retries, deduplication, and fault tolerance

Documentation & Knowledge Share
-------------------------------

-   Technical Documentation

    -   Setup instructions, API references, configuration files

    -   Supported data formats and sources

-   User Guide

    -   Step-by-step guide for deploying and testing

    -   Screenshots, sample inputs/outputs

* * * * *

Prize
-----

CAL FIRE has secured a $50,000 cash prize from the Gordon and Betty Moore Foundation, administrated by the Earth Fire Alliance, for the winner of the challenge.

* * * * *

Judging Criteria and Methodology
--------------------------------

Overview
--------

This challenge focuses on data management, storage, and consumption. Each deliverable is judged for completeness, functionality, and quality. A numeric scale will be used for evaluation.

Judging Panel:\
Scott Gregory, Phil SeLegue, Ben Rogers, Ann Kapusta, Brian Collins, Sean McFadden, Chris Anthony\
Estimated participants: ~100

* * * * *

Challenge 1: Data Sources and Ingestion Mechanisms
--------------------------------------------------

Objective
---------

Architect, design, develop, and prototype a versatile data ingestion mechanism to handle batch, real-time, and streaming data from various sources.

* * * * *

Deliverables and Scoring
------------------------

Architectural Blueprint

-   High-level system architecture diagram (0--50 points)

-   Data flow and component interaction overview (0--10 points)

-   Justification of chosen technologies for latency/fidelity (0--10 points)

Data Ingestion Prototype

-   Source adapters/connectors for batch, real-time, streaming (0--10 points)

-   Support for multiple data formats: structured, semi-structured, unstructured (0--10 points)

-   Scalable pipeline implementation (0--10 points)

Latency & Fidelity Metrics Dashboard

-   Visualization of data processing latency (0--50 points)

-   Fidelity checks and validation results (0--10 points)

Reliability & Scalability Assets

-   Error Handling & Validation Framework (0--10 points)

-   Data quality assurance modules (0--10 points)

-   Protocols for schema validation, retries, deduplication, fault tolerance (0--10 points)

Documentation & Knowledge Share

-   Technical documentation (0--10 points)

-   Setup instructions, API references, configuration files (0--10 points)

-   Supported data formats and sources (0--10 points)

-   User guide (0--10 points)

-   Step-by-step deployment/testing guide (0--10 points)

-   Screenshots, sample inputs/outputs (0--10 points)

* * * * *

Methodology:\
Each deliverable is judged on a point scale. The total possible score for Challenge 1 is 250 points.. CAL FIRE, the California Department of Forestry and Fire

Protection, stands at the forefront of needing such advanced data architectures to eﬀectively monitor,

manage, and mitigate wildfires across the state.

As CAL FIRE integrates satellite and space sensor data into their operations, they face three critical

challenges in data architecture. First, the diversity and volume of data sources and ingestion mechanisms

require eﬃcient and scalable solutions to handle real-time data streams. Second, the data storage and

processing layers must be designed to manage vast amounts of data while ensuring data integrity,

security, and accessibility. Lastly, the data consumption and analytics layers need to facilitate advanced

analytics and decision-making processes, enabling CAL FIRE to respond swiftly and eﬀectively to wildfire

threats. These challenges highlight the necessity for a comprehensive and resilient data architecture that

can support CAL FIRE's mission in safeguarding California's landscapes and communities.

Challenge 1: Data Sources and Ingestion Mechanisms

Objective: Architect, design, develop and prototype a versatile data ingestion mechanism that can

handle batch, real-time, and streaming data from various sources, ensuring minimal latency and

maximum fidelity.

In the realm of space-based data acquisition, one of the foremost challenges is to design and implement

robust data ingestion mechanisms that can eﬀectively handle a multitude of data sources and types. This

challenge necessitates the development of versatile systems capable of processing batch data, real-time

data streams, and continuous data inputs with minimal latency and maximum fidelity.

The ingestion mechanisms need to accommodate various data formats and sources, including structured

data from databases, semi-structured data from logs and APIs, and unstructured data. It is essential to

implement error handling and data validation protocols to maintain data quality throughout the

ingestion process. This includes designing scalable data pipelines that can adapt to varying data loads.

Deliverables: CAL FIRE will accept submissions starting Friday, August 22, 2025, and ending at midnight

on Sunday, October 26, 2025.Core Technical Deliverables

-  Architectural Blueprint

o  High-level system architecture diagram

o  Data flow and component interaction overview

o  Justification of chosen technologies for latency/fidelity balance

-  Data Ingestion Prototype

o  Source adapters/connectors for batch, real-time, and streaming inputs

o  Support for multiple data formats: structured, semi-structured, unstructured

o  Implementation of scalable pipelines

-  Latency & Fidelity Metrics Dashboard

o  Visualization of data processing latency across ingestion modes

o  Fidelity checks and validation results for ingested data

Reliability & Scalability Assets

-  Error Handling & Validation Framework

o  Data quality assurance modules

o  Protocols for schema validation, retries, deduplication, and fault tolerance

Documentation & Knowledge Share

-  Technical Documentation

o  Setup instructions, API references, configuration files

o  Details on supported data formats and sources

-  User Guide

o  Step-by-step guide for deploying and testing the mechanism

o  Screenshots, sample inputs/outputsPrize for Challenges

CAL FIRE has secured a $50,000 cash prize from the Gordon and Betty Moore Foundation that will be

paid by the Earth Fire Alliance for the winner of the challenge.

Judging Criteria and Methodology for Challenge

Introduction

The following document outlines the judging criteria and methodology for evaluating the deliverables of

three distinct challenges. Each challenge focuses on a diﬀerent aspect of data management, storage, and

consumption. The judging criteria are designed to ensure that teams meet all the requirements of the

deliverables in each challenge. A numeric scale will be used to judge each deliverable. The challenge will

be judged by [Scott Gregory, Phil SeLegue, Ben Rogers, Ann Kapusta, Brian Collins, Sean McFadden, and

Chris Anthony. CAL FIRE has not sponsored challenges like this before but based on anecdotal evidence

and discussions with industry CAL FIRE expects approximately 100 participants.

Challenge 1: Data Sources and Ingestion Mechanisms

Objective

Architect, design, develop, and prototype a versatile data ingestion mechanism that can handle batch,

real-time, and streaming data from various sources, ensuring minimal latency and maximum fidelity.

Deliverables and Judging Criteria - Architectural Blueprint

-  High-level system architecture diagram (0-50 points)

-  Data flow and component interaction overview (0-10 points)

-  Justification of chosen technologies for latency/fidelity balance (0-10 points)

Data Ingestion Prototype

-  Source adapters/connectors for batch, real-time, and streaming inputs (0-10 points)

-  Support for multiple data formats: structured, semi-structured, unstructured (0-10 points)

-  Implementation of scalable pipelines (0-10 points)

Latency & Fidelity Metrics Dashboard

-  Visualization of data processing latency across ingestion modes (0-50 points)

-  Fidelity checks and validation results for ingested data (0-10 points)

Reliability & Scalability Assets

-  Error Handling & Validation Framework (0-10 points)

-  Data quality assurance modules (0-10 points)

-  Protocols for schema validation, retries, deduplication, and fault tolerance (0-10 points)Documentation & Knowledge Share

-  Technical Documentation (0-10 points)

-  Setup instructions, API references, configuration files (0-10 points)

-  Details on supported data formats and sources (0-10 points)

-  User Guide (0-10 points)

-  Step-by-step guide for deploying and testing the mechanism (0-10 points)

-  Screenshots, sample inputs/outputs (0-10 points)

Methodology

Each deliverable will be judged on a scale of points based on the completeness, functionality, and quality

of the submission. The total score for Challenge 1 will be the sum of the scores for each deliverable, with

a maximum possible score of 250 points.